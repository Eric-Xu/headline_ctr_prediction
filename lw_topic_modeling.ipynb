{
 "metadata": {
  "name": "",
  "signature": "sha256:5afee3a5a7bd4fd6b78731c695a761224ca0920e4b8bb76fa1d29cf2591cf369"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from gensim import corpora, models, similarities\n",
      "\n",
      "# source: http://radimrehurek.com/gensim/tut3.html\n",
      "# source: 15_NLP/gensim_demo.ipynb\n",
      "# source: https://github.com/ga-students/DS-LA-03/blob/master/src/lesson15/gensim_demo.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 243
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = pd.read_csv(\"datasets/livewire_hits2.csv\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 204
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>title</th>\n",
        "      <th>url</th>\n",
        "      <th>pub_date</th>\n",
        "      <th>day_0</th>\n",
        "      <th>day_1</th>\n",
        "      <th>day_2</th>\n",
        "      <th>day_3</th>\n",
        "      <th>day_4</th>\n",
        "      <th>day_5</th>\n",
        "      <th>day_6</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> Ancient Crimean Gold In Legal Limbo After Russ...</td>\n",
        "      <td> http://talkingpointsmemo.com/livewire/crimean-...</td>\n",
        "      <td> 2014-03-26T18:28:25</td>\n",
        "      <td>  1332</td>\n",
        "      <td>  103</td>\n",
        "      <td>   0</td>\n",
        "      <td>   0</td>\n",
        "      <td>   0</td>\n",
        "      <td>   0</td>\n",
        "      <td>   0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> California Sec. Of State Candidate Arrested In...</td>\n",
        "      <td> http://talkingpointsmemo.com/livewire/leland-y...</td>\n",
        "      <td> 2014-03-26T18:29:17</td>\n",
        "      <td> 10743</td>\n",
        "      <td> 1483</td>\n",
        "      <td> 136</td>\n",
        "      <td>  68</td>\n",
        "      <td>  54</td>\n",
        "      <td>  38</td>\n",
        "      <td>  32</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> Planet Biden: Scientists Nickname New Dwarf Pl...</td>\n",
        "      <td> http://talkingpointsmemo.com/livewire/dwarf-pl...</td>\n",
        "      <td> 2014-03-26T18:58:47</td>\n",
        "      <td>  4907</td>\n",
        "      <td> 2039</td>\n",
        "      <td> 288</td>\n",
        "      <td> 137</td>\n",
        "      <td> 107</td>\n",
        "      <td>  87</td>\n",
        "      <td>  74</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> Mass. Guv Candidate Passes Kidney Stone During...</td>\n",
        "      <td> http://talkingpointsmemo.com/livewire/mass-guv...</td>\n",
        "      <td> 2014-03-26T19:29:00</td>\n",
        "      <td> 27494</td>\n",
        "      <td> 7345</td>\n",
        "      <td> 740</td>\n",
        "      <td> 396</td>\n",
        "      <td> 295</td>\n",
        "      <td> 213</td>\n",
        "      <td> 100</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> Charlotte's Brand New Mayor Turns Out To Be A ...</td>\n",
        "      <td> http://talkingpointsmemo.com/livewire/patrick-...</td>\n",
        "      <td> 2014-03-26T19:44:39</td>\n",
        "      <td>  6747</td>\n",
        "      <td> 1580</td>\n",
        "      <td> 127</td>\n",
        "      <td>  54</td>\n",
        "      <td>  36</td>\n",
        "      <td>  27</td>\n",
        "      <td>  22</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 205,
       "text": [
        "                                               title  \\\n",
        "0  Ancient Crimean Gold In Legal Limbo After Russ...   \n",
        "1  California Sec. Of State Candidate Arrested In...   \n",
        "2  Planet Biden: Scientists Nickname New Dwarf Pl...   \n",
        "3  Mass. Guv Candidate Passes Kidney Stone During...   \n",
        "4  Charlotte's Brand New Mayor Turns Out To Be A ...   \n",
        "\n",
        "                                                 url             pub_date  \\\n",
        "0  http://talkingpointsmemo.com/livewire/crimean-...  2014-03-26T18:28:25   \n",
        "1  http://talkingpointsmemo.com/livewire/leland-y...  2014-03-26T18:29:17   \n",
        "2  http://talkingpointsmemo.com/livewire/dwarf-pl...  2014-03-26T18:58:47   \n",
        "3  http://talkingpointsmemo.com/livewire/mass-guv...  2014-03-26T19:29:00   \n",
        "4  http://talkingpointsmemo.com/livewire/patrick-...  2014-03-26T19:44:39   \n",
        "\n",
        "   day_0  day_1  day_2  day_3  day_4  day_5  day_6  \n",
        "0   1332    103      0      0      0      0      0  \n",
        "1  10743   1483    136     68     54     38     32  \n",
        "2   4907   2039    288    137    107     87     74  \n",
        "3  27494   7345    740    396    295    213    100  \n",
        "4   6747   1580    127     54     36     27     22  "
       ]
      }
     ],
     "prompt_number": 205
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x_train = df.title[0:100]\n",
      "y_train = df.day_0[0:100]\n",
      "\n",
      "x_test = df.title[100:200]\n",
      "y_test = df.day_0[100:200]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 206
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tm_train = pd.DataFrame(columns = [\"title\", \"day_0_count\"])\n",
      "tm_train.title = x_train\n",
      "tm_train.day_0_count = y_train\n",
      "tm_train.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>title</th>\n",
        "      <th>day_0_count</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> Ancient Crimean Gold In Legal Limbo After Russ...</td>\n",
        "      <td>  1332</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> California Sec. Of State Candidate Arrested In...</td>\n",
        "      <td> 10743</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> Planet Biden: Scientists Nickname New Dwarf Pl...</td>\n",
        "      <td>  4907</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> Mass. Guv Candidate Passes Kidney Stone During...</td>\n",
        "      <td> 27494</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> Charlotte's Brand New Mayor Turns Out To Be A ...</td>\n",
        "      <td>  6747</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 207,
       "text": [
        "                                               title  day_0_count\n",
        "0  Ancient Crimean Gold In Legal Limbo After Russ...         1332\n",
        "1  California Sec. Of State Candidate Arrested In...        10743\n",
        "2  Planet Biden: Scientists Nickname New Dwarf Pl...         4907\n",
        "3  Mass. Guv Candidate Passes Kidney Stone During...        27494\n",
        "4  Charlotte's Brand New Mayor Turns Out To Be A ...         6747"
       ]
      }
     ],
     "prompt_number": 207
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Remove common words and tokenize"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stoplist = set('for a of the and to in'.split())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 208
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "texts = [[word for word in title.lower().split() if word not in stoplist] for title in tm_train.title]\n",
      "\n",
      "for t in texts[0:5]:\n",
      "    print t"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['ancient', 'crimean', 'gold', 'legal', 'limbo', 'after', \"russia's\", 'invasion']\n",
        "['california', 'sec.', 'state', 'candidate', 'arrested', 'connection', 'with', 'massive', 'fbi', 'raid']\n",
        "['planet', 'biden:', 'scientists', 'nickname', 'new', 'dwarf', 'planet', 'after', 'vp']\n",
        "['mass.', 'guv', 'candidate', 'passes', 'kidney', 'stone', 'during', 'debate']\n",
        "[\"charlotte's\", 'brand', 'new', 'mayor', 'turns', 'out', 'be', 'lemon']\n"
       ]
      }
     ],
     "prompt_number": 209
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Remove words that appear only once"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_tokens = []\n",
      "for list in texts:\n",
      "    all_tokens = all_tokens + list\n",
      "\n",
      "print len(all_tokens)\n",
      "all_tokens[0:20]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "918\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 210,
       "text": [
        "['ancient',\n",
        " 'crimean',\n",
        " 'gold',\n",
        " 'legal',\n",
        " 'limbo',\n",
        " 'after',\n",
        " \"russia's\",\n",
        " 'invasion',\n",
        " 'california',\n",
        " 'sec.',\n",
        " 'state',\n",
        " 'candidate',\n",
        " 'arrested',\n",
        " 'connection',\n",
        " 'with',\n",
        " 'massive',\n",
        " 'fbi',\n",
        " 'raid',\n",
        " 'planet',\n",
        " 'biden:']"
       ]
      }
     ],
     "prompt_number": 210
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tokens_once = set(word for word in set(all_tokens) if all_tokens.count(word) == 1)\n",
      "\n",
      "print len(tokens_once)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "517\n"
       ]
      }
     ],
     "prompt_number": 211
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "freq_texts = [[word for word in text if word not in tokens_once]\n",
      "          for text in texts]\n",
      "\n",
      "print len(freq_texts)\n",
      "freq_texts[0:20]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "100\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 212,
       "text": [
        "[['legal', 'limbo', 'after'],\n",
        " ['california', 'sec.', 'state', 'candidate', 'with'],\n",
        " ['planet', 'new', 'planet', 'after'],\n",
        " ['guv', 'candidate', 'passes', 'during', 'debate'],\n",
        " ['new', 'mayor', 'out', 'be'],\n",
        " ['gop', 'candidate', 'shoots', 'obamacare', 'new', 'campaign', 'video'],\n",
        " ['pundit', 'thinks', 'secret', 'service', 'obama'],\n",
        " ['dem', 'rep.', 'calls', 'bill', \"o'reilly's\", 'comment'],\n",
        " ['be', 'with'],\n",
        " ['man', 'himself', 'out', 'washington'],\n",
        " [],\n",
        " ['watch', 'from', '(video)'],\n",
        " ['california', 'sec.', 'state', 'candidate', 'up'],\n",
        " ['bill', 'tax', 'break'],\n",
        " ['dems', 'out', 'iowa', 'after', 'comment'],\n",
        " ['like', '(video)'],\n",
        " ['have', 'special', 'obama'],\n",
        " ['nate', \"won't\", 'end'],\n",
        " [\"christie's\", 'legal', 'review', 'bridge', 'scandal'],\n",
        " ['me', 'spying']]"
       ]
      }
     ],
     "prompt_number": 212
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Turn each document (headline) into a vectorized bag-of-words"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Creating a dictionary to map words to integers.\n",
      "lw_dict = corpora.Dictionary(freq_texts)\n",
      "print lw_dict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Dictionary(140 unique tokens: [u'senators', u'pope', u\"don't\", u'over', u'rep.']...)\n"
       ]
      }
     ],
     "prompt_number": 213
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print lw_dict.token2id.items()[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(u'senators', 133), (u'pope', 60), (u\"don't\", 116), (u'over', 69), (u'rep.', 32), (u\"won't\", 49), (u'talks', 125), (u'still', 70), (u'mayor', 15), (u'candidate', 4)]\n"
       ]
      }
     ],
     "prompt_number": 214
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "new_doc = \"Pope visits NJ mayor\"\n",
      "new_vec = lw_dict.doc2bow(new_doc.lower().split())\n",
      "print new_vec"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(15, 1), (60, 1)]\n"
       ]
      }
     ],
     "prompt_number": 215
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Vectorize: turn each document (list-of-words) into a vectorized bag-of-words.\n",
      "lw_corpus = [lw_dict.doc2bow(text) for text in texts]\n",
      "\n",
      "print len(lw_corpus)\n",
      "lw_corpus[0:20]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "100\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 216,
       "text": [
        "[[(0, 1), (1, 1), (2, 1)],\n",
        " [(3, 1), (4, 1), (5, 1), (6, 1), (7, 1)],\n",
        " [(0, 1), (8, 1), (9, 2)],\n",
        " [(4, 1), (10, 1), (11, 1), (12, 1), (13, 1)],\n",
        " [(8, 1), (14, 1), (15, 1), (16, 1)],\n",
        " [(4, 1), (8, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1)],\n",
        " [(22, 1), (23, 1), (24, 1), (25, 1), (26, 1)],\n",
        " [(27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1)],\n",
        " [(7, 1), (14, 1)],\n",
        " [(16, 1), (33, 1), (34, 1), (35, 1)],\n",
        " [],\n",
        " [(36, 1), (37, 1), (38, 1)],\n",
        " [(3, 1), (4, 1), (5, 1), (6, 1), (39, 1)],\n",
        " [(27, 1), (40, 1), (41, 1)],\n",
        " [(0, 1), (16, 1), (29, 1), (42, 1), (43, 1)],\n",
        " [(36, 1), (44, 1)],\n",
        " [(22, 1), (45, 1), (46, 1)],\n",
        " [(47, 1), (48, 1), (49, 1)],\n",
        " [(1, 1), (50, 1), (51, 1), (52, 1), (53, 1)],\n",
        " [(54, 1), (55, 1)]]"
       ]
      }
     ],
     "prompt_number": 216
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"tm_train length: \", len(tm_train)\n",
      "print \"freq_text length: \", len(freq_texts)\n",
      "print \"dictionary length: \", len(lw_dict)\n",
      "print \"corpus length: \", len(lw_corpus)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "tm_train length:  100\n",
        "freq_text length:  100\n",
        "dictionary length:  140\n",
        "corpus length:  100\n"
       ]
      }
     ],
     "prompt_number": 217
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Bag-of-words to Tf-IDF"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Transformations, from one vector representation to another.\n",
      "# E.g. bag-of-words to Tf-IDF\n",
      "# step 1 -- initialize a model. This learns document frequencies.\n",
      "tfidf = models.TfidfModel(lw_corpus)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 218
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# You can transform a vectorized bag-of-words:\n",
      "doc_bow = [(0, 5), (1, 1)]\n",
      "print(tfidf[doc_bow])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(0, 0.9675455083288751), (1, 0.2526968328108184)]\n"
       ]
      }
     ],
     "prompt_number": 219
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# You can also transform a whole corpus.\n",
      "# Note that corpus_tfidf is actually an iterator over corpus\n",
      "# that performs the transformation at the last-minute.\n",
      "# step 2 -- use the model to transform bunch of vectors.\n",
      "corpus_tfidf = tfidf[lw_corpus]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 220
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"corpus_tsidf length: \", len(corpus_tfidf)\n",
      "# for c in corpus_tfidf:\n",
      "#     print c"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "corpus_tsidf length:  100\n"
       ]
      }
     ],
     "prompt_number": 221
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Latent Sementic Analysis (LSA)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Latent semantic indexing (LSI) is an indexing and retrieval method that uses a mathematical technique called singular value decomposition (SVD) to identify patterns in the relationships between the terms and concepts contained in an unstructured collection of text. LSI is based on the principle that words that are used in the same contexts tend to have similar meanings."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# initialize an LSI transformation to define a 2D LSI space; \n",
      "# notice we apply the tf-idf transform to the term-document matrix. This generally tends to help improve results with LSA.\n",
      "lw_lsi = models.LsiModel(corpus_tfidf, id2word=lw_dict, num_topics=10)\n",
      "# lw_lsi = models.LsiModel(lw_corpus, id2word=lw_dict, num_topics=10)\n",
      "\n",
      "# create a double wrapper over the original corpus: bow->tfidf->fold-in-lsi\n",
      "# not quite sure why this is needed?\n",
      "corpus_lsi = lw_lsi[corpus_tfidf]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 296
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Topic modeling with LDA"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# initialize an LDA transformation to define a 2D LDA space; \n",
      "# unlike LSA, here we use word counts instead of tf-idf\n",
      "lw_lda = models.ldamodel.LdaModel(lw_corpus, id2word=lw_dict, num_topics=20, iterations=10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
       ]
      }
     ],
     "prompt_number": 297
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# print the most contributing words for n randomly selected topics\n",
      "for topic in lw_lda.print_topics(3):\n",
      "    print topic"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.108*know + 0.055*christie + 0.055*it's + 0.055*early + 0.055*what + 0.055*christie: + 0.055*don't + 0.034*fix + 0.034*house + 0.033*medicare\n",
        "0.062*(video) + 0.044*from + 0.038*planet + 0.038*with + 0.038*obama + 0.028*wants + 0.028*after + 0.026*senate + 0.025*dems + 0.025*candidate\n",
        "0.082*after + 0.082*him + 0.082*apologizes + 0.082*state + 0.025*critics + 0.025*about + 0.023*nate + 0.021*silver + 0.021*(video) + 0.021*he's\n"
       ]
      }
     ],
     "prompt_number": 298
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### LSA vs LDA\n",
      "LDA isn\u2019t necessarily Bayesian (at least not the original variational algorithm \u2014 it produces point estimates). In my view, the dispute is really about those who want a probabilistic model (LDA) versus those that don\u2019t care (LSA). Unfortunately, the probabilistic model produced by LDA is only of limited use, because it doesn\u2019t model the document length. But it does have clear probabilistic semantics."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Similarity Queries"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now suppose a user typed in the query \u201cHuman computer interaction\u201d. We would like to sort our nine corpus documents in decreasing order of relevance to this query. Unlike modern search engines, here we only concentrate on a single aspect of possible similarities\u2014on apparent semantic relatedness of their texts (words). No hyperlinks, no random-walk static ranks, just a semantic extension over the boolean keyword match:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "query = \"Deval Patrick: Schumer Was Wrong To Claim Timing Was Off On Obamacare\"\n",
      "\n",
      "# convert query into bag-of-words\n",
      "query_bow = lw_dict.doc2bow(query.lower().split())\n",
      "# print \"query bow \", query_bow\n",
      "# print \"-\" * 10\n",
      "\n",
      "# convert bow into LSI space\n",
      "query_lsi = lw_lsi[query_bow]\n",
      "# for qlsi in query_lsi:\n",
      "#     print \"query lsi \", qlsi\n",
      "# print \"-\" * 10\n",
      "\n",
      "# convert bow into LDA space\n",
      "query_lda = lw_lda[query_bow]\n",
      "# for qlda in query_lda:\n",
      "#     print \"query lda \", qlda"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 313
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Initializing query structures\n",
      "To prepare for similarity queries, we need to enter all documents which we want to compare against subsequent queries."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Uses cosine measure\n",
      "# Transform corpus to LDA space and index it\n",
      "lda_index = similarities.MatrixSimilarity(lw_lda[lw_corpus])\n",
      "lsi_index = similarities.MatrixSimilarity(lw_lsi[lw_corpus])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:gensim.similarities.docsim:scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:gensim.similarities.docsim:scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n"
       ]
      }
     ],
     "prompt_number": 314
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Performing LSA queries\n",
      "Cosine measure returns similarities in the range <-1, 1> (the greater, the more similar)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# perform a similarity query against the corpus\n",
      "lsi_sims = lsi_index[query_lsi]\n",
      "\n",
      "for sim in lsi_sims[0:10]:\n",
      "    print sim"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "-0.293181\n",
        "-0.0743347\n",
        "-0.019032\n",
        "0.205168\n",
        "0.207605\n",
        "0.646043\n",
        "0.320512\n",
        "0.108651\n",
        "0.00739259\n",
        "0.00922098\n"
       ]
      }
     ],
     "prompt_number": 315
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lsi_sims_sorted = sorted(enumerate(lsi_sims), key=lambda item: -item[1]) # reverse=True\n",
      "\n",
      "print \"LSA Query results for:\", query\n",
      "print \"-\" * 10\n",
      "for sim in lsi_sims_sorted[0:10]:\n",
      "    idx = sim[0]\n",
      "    print \"%0.2f %s\" % (sim[1], tm_train.ix[idx,0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "LSA Query results for: Deval Patrick: Schumer Was Wrong To Claim Timing Was Off On Obamacare\n",
        "----------\n",
        "0.90 Joe Manchin Wants To Vote On Obamacare Changes\n",
        "0.82 Louie Gohmert Complains About GOP Leaders' Tactics On Doc Fix\n",
        "0.80 Top GOPer Accuses Admin. Of 'Cooking The Books' On Health Care Sign-Ups\n",
        "0.70 Miss. Anchor Says He's Sick Of LGBT News, Tells Activists To Go On 'Gaycation'\n",
        "0.66 Kerry, Russian Counterpart Meet On Ukraine Crisis\n",
        "0.66 AUDIO: Supreme Court Oral Arguments On Birth Control Case\n",
        "0.65 GOP Candidate *Literally* Shoots Holes In Obamacare In New Campaign Video\n",
        "0.64 Long Lines As Obamacare Enrollment Deadline Fast Approaches (PHOTOS)\n",
        "0.62 Attorney For Former Christie Aide Unloads On Governor's Self-Exoneration\n",
        "0.61 GOP Senators Call For Special Committee To Investigate Benghazi\n"
       ]
      }
     ],
     "prompt_number": 316
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Performing LDA queries"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# perform a similarity query against the corpus\n",
      "lda_sims = lda_index[query_lda]\n",
      "\n",
      "for sim in lda_sims[0:10]:\n",
      "    print sim"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.0522721\n",
        "0.0238719\n",
        "0.0374606\n",
        "0.0168817\n",
        "0.0374606\n",
        "0.0168817\n",
        "0.0168817\n",
        "0.0168817\n",
        "0.0573217\n",
        "0.0374606\n"
       ]
      }
     ],
     "prompt_number": 317
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lda_sims_sorted = sorted(enumerate(lda_sims), key=lambda item: -item[1]) # reverse=True\n",
      "\n",
      "print \"LDA Query results for:\", query\n",
      "print \"-\" * 10\n",
      "for sim in lda_sims_sorted[0:10]:\n",
      "    idx = sim[0]\n",
      "    print \"%0.2f %s\" % (sim[1], tm_train.ix[idx,0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "LDA Query results for: Deval Patrick: Schumer Was Wrong To Claim Timing Was Off On Obamacare\n",
        "----------\n",
        "0.84 West Virginia Guv Vetoes Dem Legislature's 20-Week Abortion Ban\n",
        "0.83 Christie Attorney Responds To Kelly Lawyer's Charges That Report Was Sexist\n",
        "0.57 Chris Christie Holds News Conference (WATCH LIVE)\n",
        "0.57 Condi Admits White House Role in CIA Interrogation Talks\n",
        "0.56 Christie's Own Lawyer Reveals Bridgegate Shocker: Governor Was Told About Closures\n",
        "0.56 White House Delights In Taunting Republicans Who Mocked Early Obamacare Numbers\n",
        "0.56 Attorney For Former Christie Aide Unloads On Governor's Self-Exoneration\n",
        "0.56 CNN Interrupts 'Reliable Sources' To Break News On Unknown Objects That May Be Trash\n",
        "0.38 Counter-Event Planned To Protest Minnesota Restaurant's Nazi-Themed Dinner\n",
        "0.38 Birther Donald Trump Again Peddles Anti-Vaccine Conspiracy Theory\n"
       ]
      }
     ],
     "prompt_number": 318
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 318
    }
   ],
   "metadata": {}
  }
 ]
}