1. Goal one is to come up with a model to classify headlines.

  - This is similar to the spam classification and twitter sentiment analysis. Models to try are MultinomialNB, BernoulliNB, and LogisticRegression. These would have to be run on each headline category. Return the top three categories ranked by prediction confidence.

    - http://nbviewer.ipython.org/github/Eric-Xu/fall_2014_lessons/blob/master/dataexplor05/exu/naive_bayes_tweets_exu.ipynb

  - Random Forest

    - http://nbviewer.ipython.org/github/ga-students/DS-LA-03/blob/master/src/lesson09/Random%20Forests.ipynb

  - Return nbest categories and their prediction accuracy:

    - lw_predict_proba

  - SVM; which is widely regarded as one of the best text classification algorithms

    - http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html

  - CountVectorizer's max_df

    - build the vocabulary by ignoring terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float, the parameter represents a proportion of documents, integer absolute counts.
    - lw_countvec

  - CountVectorizer's min_df

    - build the vocabulary by ignoring terms that have a document frequency strictly lower than the given threshold.
    - lw_bayes

  - NLTK's Porter stemmer

    - lw_pos

  - TODO/Additional goals:

    - Stacked Generalization

      - http://www.chioka.in/stacking-blending-and-stacked-generalization/
      - https://github.com/log0/vertebral/blob/master/stacked_generalization.py

    - Streamline models by using Pipelines and FeatureUnion

      - http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html
      - https://github.com/zacstewart/kaggle_seeclickfix/blob/master/estimator.py
      - http://nbviewer.ipython.org/github/rasbt/pattern_classification/blob/master/machine_learning/scikit-learn/scikit-pipeline.ipynb
      - http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html


2. Goal two is to come up with a model to predict page views given a headline.

  - Option 1 is to run a linear regression model to predict actual page views.

    - try Vowpal Wabbit; http://fastml.com/predicting-advertised-salaries/

    - check if page views has a normal distribution. If the distribution is skewed, then normalize it by taking the log value, since linear models expect the residuals to have a normal distribution. (http://fastml.com/predicting-advertised-salaries/)

    - k-means cannot handle headlines as strings

    - feature selection choices

      - forward feature selection:

        - http://nbviewer.ipython.org/github/rasbt/algorithms_in_ipython_notebooks/blob/master/ipython_nbs/machine_learning/sequential_selection_algorithms.ipynb#sections
        - http://cs229.stanford.edu/proj2012/MahanaJohnsApte-AutomatedEssayGradingUsingMachineLearning.pdf

      - SelectKBest: dataexplor04/exu/diabetes_exu2.ipynb

  - Option 2 is to run a LogReg or NaiveBayes model to predict which page-view-quartile a headline is likely to fall under. This would require converting the raw page views into performance quartiles.

    - lw_bayes.ipynb

      - quartiles performed better than deciles

      - CountVectorizer (n-gram 2) did better than a TF-IDF representation


  - Additional goals:

    - use category_hat as an additional predictor

      - http://nbviewer.ipython.org/github/Eric-Xu/fall_2014_lessons/blob/master/dataexplor05/exu/dataexplor05_sample_new_exu.ipynb

  - Option 3 is to use topic modeling (LSA/LDA) to search for the most similar headlines

    - if the new headlines is semantically similar to the returned results, then use the pageviews of the top result as a feature.

    - lw_topic_modeling.ipynb